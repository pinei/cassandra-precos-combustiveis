{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=262006177488-3425ks60hkk80fssi9vpohv88g6q1iqd.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform&state=yV0TRmbJs2qxVprdMEfXjINh2xEa0t&prompt=consent&access_type=offline\n",
      "\n",
      "...\n",
      "dataset_id: \n",
      "\tbr_anp_precos_combustiveis \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "import basedosdados as bd\n",
    "\n",
    "bd.list_datasets() # deve listar entre os datasets: 'br_anp_precos_combustiveis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "table_id: \n",
      "\tmicrodados \n",
      "-----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bd.list_dataset_tables('br_anp_precos_combustiveis') # deve listar 'microdados'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "BILLING_PROJECT_ID = os.getenv(\"BILLING_PROJECT_ID\")\n",
    "\n",
    "import basedosdados as bd\n",
    "\n",
    "QUERY_PROJECT_ID = 'basedosdados'\n",
    "DATASET_ID = 'br_anp_precos_combustiveis'\n",
    "TABLE_ID = 'microdados'\n",
    "\n",
    "def fetch_precos_combustiveis(ano):\n",
    "    query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM `{QUERY_PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`\n",
    "    WHERE ano = {ano}\n",
    "    \"\"\"\n",
    "    return bd.read_sql(query, billing_project_id=BILLING_PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/sdkman/candidates/spark/3.5.0/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Crie uma sessão do Spark\n",
    "spark = ( \n",
    "    SparkSession.builder.appName(\"Cassandra\")\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.4.1\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão do Spark: 3.5.0\n",
      "Configurações Spark:\n",
      "spark.jars.packages : com.datastax.spark:spark-cassandra-connector_2.12:3.4.1\n",
      "spark.app.name : Cassandra\n",
      "spark.sql.warehouse.dir : file:/workspaces/cassandra-precos-combustiveis/spark-warehouse\n",
      "spark.executor.id : driver\n",
      "spark.app.submitTime : 1702312679253\n",
      "spark.app.initial.jar.urls : spark://ed53c209cc3c:42367/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar,spark://ed53c209cc3c:42367/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar,spark://ed53c209cc3c:42367/jars/org.apache.commons_commons-lang3-3.10.jar,spark://ed53c209cc3c:42367/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar,spark://ed53c209cc3c:42367/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar,spark://ed53c209cc3c:42367/jars/org.scala-lang_scala-reflect-2.12.11.jar,spark://ed53c209cc3c:42367/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar,spark://ed53c209cc3c:42367/jars/com.thoughtworks.paranamer_paranamer-2.8.jar,spark://ed53c209cc3c:42367/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar,spark://ed53c209cc3c:42367/jars/org.reactivestreams_reactive-streams-1.0.3.jar,spark://ed53c209cc3c:42367/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar,spark://ed53c209cc3c:42367/jars/com.datastax.oss_native-protocol-1.5.0.jar,spark://ed53c209cc3c:42367/jars/com.google.code.findbugs_jsr305-3.0.2.jar,spark://ed53c209cc3c:42367/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar,spark://ed53c209cc3c:42367/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar,spark://ed53c209cc3c:42367/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar,spark://ed53c209cc3c:42367/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar,spark://ed53c209cc3c:42367/jars/org.slf4j_slf4j-api-1.7.26.jar,spark://ed53c209cc3c:42367/jars/com.typesafe_config-1.4.1.jar\n",
      "spark.driver.host : ed53c209cc3c\n",
      "spark.app.id : local-1702312683921\n",
      "spark.jars : file:///home/vscode/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar,file:///home/vscode/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar,file:///home/vscode/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar,file:///home/vscode/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar,file:///home/vscode/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar,file:///home/vscode/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar,file:///home/vscode/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar,file:///home/vscode/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar,file:///home/vscode/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar,file:///home/vscode/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar,file:///home/vscode/.ivy2/jars/com.typesafe_config-1.4.1.jar,file:///home/vscode/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar,file:///home/vscode/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar,file:///home/vscode/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar,file:///home/vscode/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar,file:///home/vscode/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar,file:///home/vscode/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar,file:///home/vscode/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar,file:///home/vscode/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar\n",
      "spark.driver.extraJavaOptions : -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.repl.local.jars : file:///home/vscode/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar,file:///home/vscode/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar,file:///home/vscode/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar,file:///home/vscode/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar,file:///home/vscode/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar,file:///home/vscode/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar,file:///home/vscode/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar,file:///home/vscode/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar,file:///home/vscode/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar,file:///home/vscode/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar,file:///home/vscode/.ivy2/jars/com.typesafe_config-1.4.1.jar,file:///home/vscode/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar,file:///home/vscode/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar,file:///home/vscode/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar,file:///home/vscode/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar,file:///home/vscode/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar,file:///home/vscode/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar,file:///home/vscode/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar,file:///home/vscode/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar\n",
      "spark.app.startTime : 1702312680221\n",
      "spark.rdd.compress : True\n",
      "spark.serializer.objectStreamReset : 100\n",
      "spark.master : local[*]\n",
      "spark.submit.pyFiles : /home/vscode/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar,/home/vscode/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar,/home/vscode/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar,/home/vscode/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar,/home/vscode/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar,/home/vscode/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar,/home/vscode/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar,/home/vscode/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar,/home/vscode/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar,/home/vscode/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar,/home/vscode/.ivy2/jars/com.typesafe_config-1.4.1.jar,/home/vscode/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar,/home/vscode/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar,/home/vscode/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar,/home/vscode/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar,/home/vscode/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar,/home/vscode/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar,/home/vscode/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar,/home/vscode/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar\n",
      "spark.submit.deployMode : client\n",
      "spark.driver.port : 42367\n",
      "spark.app.initial.file.urls : file:///home/vscode/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar,file:///home/vscode/.ivy2/jars/com.typesafe_config-1.4.1.jar,file:///home/vscode/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar,file:///home/vscode/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar,file:///home/vscode/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar,file:///home/vscode/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar,file:///home/vscode/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar,file:///home/vscode/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar,file:///home/vscode/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar,file:///home/vscode/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar,file:///home/vscode/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar,file:///home/vscode/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar,file:///home/vscode/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar,file:///home/vscode/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar,file:///home/vscode/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar,file:///home/vscode/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar,file:///home/vscode/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar,file:///home/vscode/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar,file:///home/vscode/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar\n",
      "spark.ui.showConsoleProgress : true\n",
      "spark.files : file:///home/vscode/.ivy2/jars/com.datastax.spark_spark-cassandra-connector_2.12-3.4.1.jar,file:///home/vscode/.ivy2/jars/com.datastax.spark_spark-cassandra-connector-driver_2.12-3.4.1.jar,file:///home/vscode/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.11.0.jar,file:///home/vscode/.ivy2/jars/com.datastax.oss_java-driver-core-shaded-4.13.0.jar,file:///home/vscode/.ivy2/jars/com.datastax.oss_java-driver-mapper-runtime-4.13.0.jar,file:///home/vscode/.ivy2/jars/org.apache.commons_commons-lang3-3.10.jar,file:///home/vscode/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.8.jar,file:///home/vscode/.ivy2/jars/org.scala-lang_scala-reflect-2.12.11.jar,file:///home/vscode/.ivy2/jars/com.datastax.oss_native-protocol-1.5.0.jar,file:///home/vscode/.ivy2/jars/com.datastax.oss_java-driver-shaded-guava-25.1-jre-graal-sub-1.jar,file:///home/vscode/.ivy2/jars/com.typesafe_config-1.4.1.jar,file:///home/vscode/.ivy2/jars/org.slf4j_slf4j-api-1.7.26.jar,file:///home/vscode/.ivy2/jars/io.dropwizard.metrics_metrics-core-4.1.18.jar,file:///home/vscode/.ivy2/jars/org.hdrhistogram_HdrHistogram-2.1.12.jar,file:///home/vscode/.ivy2/jars/org.reactivestreams_reactive-streams-1.0.3.jar,file:///home/vscode/.ivy2/jars/com.github.stephenc.jcip_jcip-annotations-1.0-1.jar,file:///home/vscode/.ivy2/jars/com.github.spotbugs_spotbugs-annotations-3.1.12.jar,file:///home/vscode/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar,file:///home/vscode/.ivy2/jars/com.datastax.oss_java-driver-query-builder-4.13.0.jar\n",
      "spark.executor.extraJavaOptions : -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n"
     ]
    }
   ],
   "source": [
    "# Obtendo a versão do Spark\n",
    "versao_spark = spark.version\n",
    "print(\"Versão do Spark:\", versao_spark)\n",
    "\n",
    "# Outras informações da sessão Spark\n",
    "print(\"Configurações Spark:\")\n",
    "for conf, val in spark.sparkContext.getConf().getAll():\n",
    "    print(conf, \":\", val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caminho do Spark: /usr/local/sdkman/candidates/spark/current\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "# Inicializando o Spark no notebook\n",
    "findspark.init()\n",
    "\n",
    "# Obtendo o caminho do Spark\n",
    "caminho_spark = findspark.find()\n",
    "print(\"Caminho do Spark:\", caminho_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit, col\n",
    "\n",
    "# Propriedades de conexão com o Cassandra\n",
    "CASSANDRA_CONF = {\n",
    "    \"spark.cassandra.connection.host\": \"cassandra\",\n",
    "    \"spark.cassandra.connection.port\": 9042,\n",
    "}\n",
    "\n",
    "def ingestao_precos_combustiveis_ano(ano):\n",
    "    print(':. Carregando o dataframe')\n",
    "    df = fetch_precos_combustiveis(ano)\n",
    "\n",
    "    print(':. Convertendo para o dataframe do Spark')\n",
    "    df_spark = spark.createDataFrame(df)\n",
    "\n",
    "    print(':. Tratando valores NaN')\n",
    "    df_spark = (\n",
    "        df_spark\n",
    "        .withColumn('preco_compra', when(col('preco_compra') == 'NaN', lit(None)).otherwise(col('preco_compra')))\n",
    "        .withColumn('preco_venda', when(col('preco_venda') == 'NaN', lit(None)).otherwise(col('preco_venda')))\n",
    "    ) \n",
    "\n",
    "    print(':. Salvando no Cassandra')\n",
    "    (\n",
    "        df_spark.write.format(\"org.apache.spark.sql.cassandra\")\n",
    "        .options(**CASSANDRA_CONF)\n",
    "        .options(keyspace=\"anp\", table=\"precos_combustiveis_por_produto_e_uf\")\n",
    "        .mode(\"append\")\n",
    "        .save()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestão de dados para o ano 2004\n",
      ":. Carregando o dataframe\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      ":. Convertendo para o dataframe do Spark\n",
      ":. Tratando valores NaN\n",
      ":. Salvando no Cassandra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestão de dados para o ano 2005\n",
      ":. Carregando o dataframe\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      ":. Convertendo para o dataframe do Spark\n",
      ":. Tratando valores NaN\n",
      ":. Salvando no Cassandra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestão de dados para o ano 2006\n",
      ":. Carregando o dataframe\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      ":. Convertendo para o dataframe do Spark\n",
      ":. Tratando valores NaN\n",
      ":. Salvando no Cassandra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestão de dados para o ano 2007\n",
      ":. Carregando o dataframe\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      ":. Convertendo para o dataframe do Spark\n",
      ":. Tratando valores NaN\n",
      ":. Salvando no Cassandra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestão de dados para o ano 2008\n",
      ":. Carregando o dataframe\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      ":. Convertendo para o dataframe do Spark\n",
      ":. Tratando valores NaN\n",
      ":. Salvando no Cassandra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestão de dados para o ano 2009\n",
      ":. Carregando o dataframe\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      ":. Convertendo para o dataframe do Spark\n",
      ":. Tratando valores NaN\n",
      ":. Salvando no Cassandra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestão de dados para o ano 2010\n",
      ":. Carregando o dataframe\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      ":. Convertendo para o dataframe do Spark\n",
      ":. Tratando valores NaN\n",
      ":. Salvando no Cassandra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestão de dados para o ano 2011\n",
      ":. Carregando o dataframe\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      ":. Convertendo para o dataframe do Spark\n",
      ":. Tratando valores NaN\n",
      ":. Salvando no Cassandra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestão de dados para o ano 2012\n",
      ":. Carregando o dataframe\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      ":. Convertendo para o dataframe do Spark\n",
      ":. Tratando valores NaN\n",
      ":. Salvando no Cassandra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestão de dados para o ano 2013\n",
      ":. Carregando o dataframe\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      ":. Convertendo para o dataframe do Spark\n",
      ":. Tratando valores NaN\n",
      ":. Salvando no Cassandra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestão de dados para o ano 2014\n",
      ":. Carregando o dataframe\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      ":. Convertendo para o dataframe do Spark\n",
      ":. Tratando valores NaN\n",
      ":. Salvando no Cassandra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestão de dados para o ano 2015\n",
      ":. Carregando o dataframe\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      ":. Convertendo para o dataframe do Spark\n",
      ":. Tratando valores NaN\n",
      ":. Salvando no Cassandra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestão de dados para o ano 2016\n",
      ":. Carregando o dataframe\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      ":. Convertendo para o dataframe do Spark\n",
      ":. Tratando valores NaN\n",
      ":. Salvando no Cassandra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestão de dados para o ano 2017\n",
      ":. Carregando o dataframe\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      ":. Convertendo para o dataframe do Spark\n",
      ":. Tratando valores NaN\n",
      ":. Salvando no Cassandra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestão de dados para o ano 2018\n",
      ":. Carregando o dataframe\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      ":. Convertendo para o dataframe do Spark\n",
      ":. Tratando valores NaN\n",
      ":. Salvando no Cassandra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestão de dados para o ano 2019\n",
      ":. Carregando o dataframe\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      ":. Convertendo para o dataframe do Spark\n",
      ":. Tratando valores NaN\n",
      ":. Salvando no Cassandra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestão de dados para o ano 2020\n",
      ":. Carregando o dataframe\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      ":. Convertendo para o dataframe do Spark\n",
      ":. Tratando valores NaN\n",
      ":. Salvando no Cassandra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestão de dados para o ano 2021\n",
      ":. Carregando o dataframe\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      ":. Convertendo para o dataframe do Spark\n",
      ":. Tratando valores NaN\n",
      ":. Salvando no Cassandra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestão de dados para o ano 2022\n",
      ":. Carregando o dataframe\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      ":. Convertendo para o dataframe do Spark\n",
      ":. Tratando valores NaN\n",
      ":. Salvando no Cassandra\n"
     ]
    }
   ],
   "source": [
    "for ano in range(2004, 2023):\n",
    "    print(f'Ingestão de dados para o ano {ano}')\n",
    "    ingestao_precos_combustiveis_ano(ano)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+\n",
      "|           produto|count(1)|\n",
      "+------------------+--------+\n",
      "|Gasolina Aditivada|  294451|\n",
      "|            Etanol| 2831126|\n",
      "|               Gnv|  181106|\n",
      "|            Diesel| 2156850|\n",
      "|        Diesel S50|    5901|\n",
      "|        Diesel S10| 1106498|\n",
      "|               Glp| 3585616|\n",
      "|          Gasolina| 3079017|\n",
      "+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    spark.read\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\n",
    "    .options(**CASSANDRA_CONF)\n",
    "    .options(keyspace=\"anp\", table=\"precos_combustiveis_por_produto_e_uf\")\n",
    "    .load()\n",
    "    .createOrReplaceTempView(\"vw_precos\")\n",
    ")\n",
    "\n",
    "spark.sql('select produto, count(*) from vw_precos group by produto').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
